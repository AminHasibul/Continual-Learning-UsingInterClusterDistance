{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preventing Forgetting in Continual Learning\n",
    "\n",
    "This notebook demonstrates how to use custom architectures and loss functions to prevent catastrophic forgetting in continual learning scenarios.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Continual learning (also known as lifelong learning or incremental learning) aims to enable machine learning models to learn from a continuous stream of data, acquiring new knowledge while retaining previously learned information.\n",
    "\n",
    "**Key Challenges:**\n",
    "- **Catastrophic Forgetting**: Neural networks tend to forget previously learned tasks when trained on new tasks\n",
    "- **Stability-Plasticity Dilemma**: Balancing the ability to learn new information while preserving old knowledge\n",
    "\n",
    "**Approaches Implemented:**\n",
    "1. Custom CNN and ResNet architectures\n",
    "2. Knowledge Distillation Loss\n",
    "3. Elastic Weight Consolidation (EWC)\n",
    "4. Learning without Forgetting (LwF)\n",
    "5. iCaRL (Incremental Classifier and Representation Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import custom modules\n",
    "from models import CustomCNN, CustomResNet, create_resnet18\n",
    "from losses import KnowledgeDistillationLoss, ElasticWeightConsolidationLoss, LwFLoss, compute_fisher_information\n",
    "from experiments import ContinualLearningBenchmark, create_task_split\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Dataset\n",
    "\n",
    "We'll use CIFAR-10 as our benchmark dataset, split into multiple tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Classes: {train_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Task Splits\n",
    "\n",
    "Split the dataset into sequential tasks (e.g., 5 tasks with 2 classes each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "num_tasks = 5\n",
    "batch_size = 128\n",
    "\n",
    "# Create task splits\n",
    "train_task_datasets = create_task_split(train_dataset, num_tasks, task_type='class_incremental')\n",
    "test_task_datasets = create_task_split(test_dataset, num_tasks, task_type='class_incremental')\n",
    "\n",
    "# Create data loaders for each task\n",
    "train_loaders = [\n",
    "    DataLoader(task_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    for task_data in train_task_datasets\n",
    "]\n",
    "\n",
    "test_loaders = [\n",
    "    DataLoader(task_data, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    for task_data in test_task_datasets\n",
    "]\n",
    "\n",
    "print(f\"Number of tasks: {num_tasks}\")\n",
    "for i, loader in enumerate(train_loaders):\n",
    "    print(f\"Task {i}: {len(loader.dataset)} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline: Training without Continual Learning\n",
    "\n",
    "First, let's establish a baseline by training sequentially without any anti-forgetting techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "baseline_model = CustomCNN(num_classes=10, input_channels=3)\n",
    "\n",
    "# Create benchmark\n",
    "baseline_benchmark = ContinualLearningBenchmark(\n",
    "    model=baseline_model,\n",
    "    device=device,\n",
    "    num_tasks=num_tasks\n",
    ")\n",
    "\n",
    "# Run benchmark\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE: Training without continual learning techniques\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "baseline_results = baseline_benchmark.run_benchmark(\n",
    "    train_loaders=train_loaders,\n",
    "    test_loaders=test_loaders,\n",
    "    epochs_per_task=10,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Print performance matrix\n",
    "baseline_benchmark.print_performance_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment 1: Custom ResNet with Knowledge Distillation\n",
    "\n",
    "Use a custom ResNet architecture with knowledge distillation to preserve old knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement knowledge distillation experiment\n",
    "# This is a template - customize based on your needs\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 1: ResNet with Knowledge Distillation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create model\n",
    "resnet_model = create_resnet18(num_classes=10, input_channels=3)\n",
    "\n",
    "# TODO: Implement training loop with knowledge distillation\n",
    "# Hint: Store old model, use KnowledgeDistillationLoss\n",
    "\n",
    "print(\"Implementation pending - add your custom code here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment 2: Elastic Weight Consolidation (EWC)\n",
    "\n",
    "Apply EWC to constrain important parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement EWC experiment\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 2: Elastic Weight Consolidation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create model\n",
    "ewc_model = CustomCNN(num_classes=10, input_channels=3)\n",
    "\n",
    "# TODO: Implement EWC training\n",
    "# Hint: Compute Fisher information after each task\n",
    "# Use ElasticWeightConsolidationLoss\n",
    "\n",
    "print(\"Implementation pending - add your custom code here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment 3: Learning without Forgetting (LwF)\n",
    "\n",
    "Use LwF to maintain performance on old tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement LwF experiment\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 3: Learning without Forgetting\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create model\n",
    "lwf_model = create_resnet18(num_classes=10, input_channels=3)\n",
    "\n",
    "# TODO: Implement LwF training\n",
    "# Hint: Use LwFLoss with old task indices\n",
    "\n",
    "print(\"Implementation pending - add your custom code here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization and Comparison\n",
    "\n",
    "Compare the performance of different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add visualization code\n",
    "\n",
    "# Example: Plot performance matrix as heatmap\n",
    "def plot_performance_matrix(performance_matrix, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(performance_matrix, annot=True, fmt='.1f', cmap='YlOrRd',\n",
    "                xticklabels=[f'Task {i}' for i in range(num_tasks)],\n",
    "                yticklabels=[f'After Task {i}' for i in range(num_tasks)])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Task Evaluated')\n",
    "    plt.ylabel('Training Stage')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot baseline results\n",
    "plot_performance_matrix(baseline_results['performance_matrix'], \n",
    "                       'Baseline Performance Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Metrics Summary\n",
    "\n",
    "Compare metrics across different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comparison table/plot\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example metrics comparison\n",
    "metrics_data = {\n",
    "    'Method': ['Baseline'],\n",
    "    'Average Accuracy': [baseline_results['final_metrics']['average_accuracy']],\n",
    "    'Forgetting': [baseline_results['final_metrics']['forgetting']],\n",
    "    'Backward Transfer': [baseline_results['final_metrics']['backward_transfer']]\n",
    "}\n",
    "\n",
    "# TODO: Add other methods' metrics\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(\"\\nMetrics Comparison:\")\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions and Next Steps\n",
    "\n",
    "### Key Findings\n",
    "- TODO: Add your observations\n",
    "- Compare baseline vs. continual learning methods\n",
    "- Analyze forgetting patterns\n",
    "\n",
    "### Future Work\n",
    "1. Experiment with different architectures\n",
    "2. Try other continual learning methods (e.g., PackNet, Progressive Neural Networks)\n",
    "3. Test on different datasets (ImageNet, MNIST, etc.)\n",
    "4. Optimize hyperparameters\n",
    "5. Implement memory replay strategies\n",
    "\n",
    "### References\n",
    "- [Learning without Forgetting](https://arxiv.org/abs/1606.09282)\n",
    "- [Elastic Weight Consolidation](https://arxiv.org/abs/1612.00796)\n",
    "- [iCaRL: Incremental Classifier and Representation Learning](https://arxiv.org/abs/1611.07725)\n",
    "- [Continual Learning Survey](https://arxiv.org/abs/1909.08383)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Model Architecture Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model architectures\n",
    "print(\"Custom CNN Architecture:\")\n",
    "print(\"=\"*60)\n",
    "cnn_model = CustomCNN(num_classes=10)\n",
    "print(cnn_model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Custom ResNet-18 Architecture:\")\n",
    "print(\"=\"*60)\n",
    "resnet_model = create_resnet18(num_classes=10)\n",
    "print(resnet_model)\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nCustom CNN parameters: {count_parameters(cnn_model):,}\")\n",
    "print(f\"ResNet-18 parameters: {count_parameters(resnet_model):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
